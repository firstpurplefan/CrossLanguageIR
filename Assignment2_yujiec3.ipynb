{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "4e43b1b4047984503d948bf08ba7a04c13b0192bca43ee4df1cce294",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# COMP90042 Assignment #2: Cross-language Information Retreival"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "signature": "46c24588e7e293e130668a8e5e47ae2cd1b7ddbdb77f2f88181a0b77"
   },
   "source": [
    "```Student Name: Yujie Cheng\n",
    "Student ID: 724408```\n",
    "\n",
    "Please do not edit this field. It must be left as is for use in marking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "9b607b8e602c14824e9397127cda2275f21021d5e1ff0cb80702dd21"
   },
   "source": [
    "## General info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "da8e68cd2e7a52ad1de13b36b937d3df7f45e01283999ba145efc0e7"
   },
   "source": [
    "**Due date**: 5pm, Wed 25th May\n",
    "\n",
    "**Submission method**: see LMS\n",
    "\n",
    "**Submission materials**: completed copy of this ipython notebook. *Do not upload the corpora*, you can assume these files will be extracted into the same folder as the code.  *Do not use absolute paths* such as paths to files on your machine, use *relative paths* instead. *Include output* in the ipython notebook, and ensure your code can run sucessfully from scratch (e.g., use *Kernel -> Restart and Run All* menu option to test.)\n",
    "\n",
    "**Late submissions**: -10% per day, no late submissions after the first week\n",
    "\n",
    "**Marks**: 25% of mark for class\n",
    "\n",
    "**Overview**: For this project, you'll be building a cross language information retreival system (CLIR), which is capable of searching for important text documents written in a English given a query in German, and displaying the results in German. This incorporates machine translation, information retrieval using a vector space model, and IR evaluation. A key focus of this project is critical analysis and experimental evaluation, for which you will need to report on the relative merits of various options. \n",
    "\n",
    "**Materials**: See the main class LMS page for information on the basic setup required for this class, including an iPython notebook viewer and the python packages as used for workshops and the previous assignments. The main part of this assignment should be done entirely in python, using built-in python packages and the packages used in the class (NLTK, Numpy, Scipy, Matplotlib, Sci-kit Learn, and Gemsim). Please do no use any other 3rd party packages. Note that some choices of the *extended part* of the assignment may require you to work with other tools, such as 3rd party translation tools or retreival engines, in which case this part of the assignment will require working in another environment, although you should report your results in the ipython notebook and attach your other files (code, shell scripts etc) with your submission.   \n",
    "\n",
    "You are encouraged to make use of the iPython notebooks released for this class as well as other online documentation to guide your responses, but you should not copy directly from any source. Adapting code from the class notebooks is permitted.\n",
    "\n",
    "**Data**: The other data you will need are:\n",
    "  - parallel German-English corpus.  This data comes from the Europarl corpus, which is a collection of debates held in the EU parliament over many years, translated into several EU languages. This data was collated as part of the annual WMT translation competitions. The data has been tokenised, sentence aligned using the Church-Gale algorithm. \n",
    "  - CLIR evaluation corpus, comprising a large collection of English documents sourced from Wikipedia along with several German query strings and relevance judgements for each document and query pair. \n",
    "  \n",
    "The data files are as follows:\n",
    "  - *bitext-small.{en,de}* is the sentence-aligned parallel corpus, of a small enough size for you to use for developing word-alignment tools. You may wish to work on a smaller subset during code development.\n",
    "  - *bitext-large.{en,de}* is a much larger version of the above corpus, for language modelling and translation.\n",
    "  - *newstest2013.{en,de}* is a separate small parallel corpus reserved for evaluation.\n",
    "  - *dev.{docs,queries,qrel}* is a set of documents in English, queries in German and query relevance judgements, for the development of the IR components and evaluation.\n",
    "  \n",
    "For details of the datasets, please see:\n",
    "> Philipp Koehn. *Europarl: A Parallel Corpus for Statistical Machine Translation* MT Summit 2005.   \n",
    "\n",
    "and\n",
    "> Shigehiko Schamoni, Felix Hieber, Artem Sokolov, Stefan Riezler. *Learning Translational and Knowledge-based Similarities from Relevance Rankings for Cross-Language Retrieval* ACL 2014.\n",
    "\n",
    "You can find the PDFs for both papers online with a quick web search.\n",
    "\n",
    "**Evaluation**: Your iPython notebook should run end-to-end without any errors in a reasonable amount of time (less than 15 minutes on a lab desktop), and you must follow all instructions provided below, including specific implementation requirements. Indexing or translation steps might taken longer than this, however you should upload with your submission preprocessed files (e.g., text files, pickled python structures) to keep the runtime modest.\n",
    "You will be marked not only on the correctness of your methods, but also on your explanation and analysis. \n",
    "\n",
    "Please do not change any of instruction text in the notebook. Where applicable, leave the output cells in the code, particularly when you are commenting on that output. You should add your answers and code by inserting a markdown cell between every major function or other block of code explaining its purpose or anywhere a result needs to be discussed (see the class notebooks for examples). Note that even if you do something wrong, you might get partial credit if you explain it enough that we can follow your reasoning, whereas a fully correct assignment with no text commentary will not receive a passing score. You will not be marked directly on the performance of your final classifier, but each of the steps you take to build it should be reasonable and well-defended.\n",
    "\n",
    "**Updates**: Any major changes to the assignment will be announced via LMS. Minor changes and clarifications will be announced in the forum on LMS, we recommend you check the forum regularly.\n",
    "\n",
    "**Academic Misconduct**: For most people, collaboration will form a natural part of the undertaking of this project, and we encourage you to discuss it in general terms with other students. However, it is still an individual task, and so reuse of code or other instances of clear influence will be considered cheating. We will be checking submissions for originality and will invoke the University’s [Academic Misconduct policy](http://academichonesty.unimelb.edu.au/policy.html) where inappropriate levels of collusion or plagiarism are deemed to have taken place.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "d3a238704f1700b3cc3e048e7f4e5f6a71a3e48993cb666a704439c1"
   },
   "source": [
    "### Warning: File encodings and tokenisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "3fbb1b1a9e6314c8c522ef37a7d2c7fa914f039d06d631d1e3259c63"
   },
   "source": [
    "All the text files are encoded in *utf-8*, which requires some care in using with python strings, the nltk tools and jupyter.  Please use the following method to convert strings into ASCII by escaping the special symbols. Be careful to do the conversion after tokenisation, as the escaped tags might interfere with the NLTK tokenizer and get treated as punctuation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "3eb756d983cf958af9898d0a51a5f421efba8a15b2ce0304a7ea28d3"
   },
   "source": [
    "# Part 1: CLIR engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "6c74a0d44eb534f669c839718d09d55a13745033782f03e6b4669458"
   },
   "source": [
    "The project has two parts: first you must build a CLIR engine, comprising information retrieval and translation components, and evaluate its accuracy. Next you will propose an extension to a component of the system. In each step you will need to justify your modelling and implementation decisions, and evaluate the quality of the outputs of the system, and at the end you will need to reflect on the overall outcomes.\n",
    "\n",
    "The CLIR system will involve:\n",
    " - translating queries from German into English, the language of our text collection, using word-based translation\n",
    " - once the queries and the documents are in the same language, search over the document collection using BM25\n",
    " - evaluate the quality of ranked retreival results using the query relevance judgements\n",
    "\n",
    "Note that you could try translating the text collection into German instead, but for this project we'll stick with translating the query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "9bfba264d3b5ea429a94f82960e7dee8410a9eb1aece6c5ce8ba807c"
   },
   "source": [
    "Building the CLIR engine is the majority of work in the assignment, and  constitutes 70% of the assignment mark. Note that although there are several steps, they do not necessarily need to be attempted in a linear order. That is, some components are independent of others. If you're struggling with one component and cannot complete it, then you may be able to skip over it and return to it later. Where outputs are needed in a subsequent step you may want to consider ways of side-stepping this need, e.g., by using Google translate output rather than your system output for translating the queries. You should aim to answer all components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "5a095f68d2c300d44953dea82ac63365579ef26e510897608eb482d7"
   },
   "source": [
    "## Information Retreival with BM25 (20% of assignment mark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "signature": "10e5614ae8fd1da58ad6e9e7cd4ccca82f06f7e419912b3f5b6cbd00"
   },
   "source": [
    "You should implement a vector-space retreival system using the BM25 approach. You'll need to:\n",
    " - load in the data files and tokenize the input,\n",
    " - preprocess the lexicon, e.g., by stemming and stop-word removal, \n",
    " - calculate the TF/IDF representation for all documents in the collection,\n",
    " - store an inverted index such that you can efficiently recover the documents matching a query term, and\n",
    " - implement querying in the BM25 model\n",
    " - test that it runs with some English queries (you'll have to make these up)\n",
    " \n",
    "This should run in a reasonable time over *dev.docs* (up to a few minutes to index), but beyond this does not need to be highly optimised. Feel free to use python dictionaries, sets, lists, numpy/scipy matrices etc where appropriate for best runtime, but you shouldn't need to use specialised data structures such as compression schemes or the like. Feel free to use APIs in NLTK, scikit-learn and other allowed python modules as needed (see list above.) You will probably want to save and load your index to/from disk to save repeated re-indexing every time you load the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "signature": "94c6b00fde9eb05fd2333ccbfddb73b3de7f71569fec4bb97fd8e78f"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "import collections\n",
    "import math\n",
    "from nltk.tokenize import word_tokenize\n",
    "from six import iteritems\n",
    "from six.moves import xrange\n",
    "\n",
    "punctuation = set(string.punctuation)\n",
    "english_stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "german_stop_words = set(nltk.corpus.stopwords.words('german'))\n",
    "\n",
    "porter_stemmer = nltk.stem.porter.PorterStemmer()\n",
    "\n",
    "def tokenize(line, tokenizer = word_tokenize):\n",
    "    utf_line = line.decode('utf-8').lower()\n",
    "    return [token.encode('ascii', 'backslashreplace') for token in tokenizer(utf_line)]\n",
    "\n",
    "def preprocess(doc):\n",
    "    terms = []\n",
    "    for token in tokenize(doc):\n",
    "        if token not in punctuation and token not in english_stop_words:\n",
    "            terms.append(token)\n",
    "    return terms\n",
    "\n",
    "def preprocess_de(doc):\n",
    "    terms = []\n",
    "    for token in tokenize(doc):\n",
    "        if token not in punctuation and token not in german_stop_words:\n",
    "            terms.append(token)\n",
    "    return terms\n",
    "\n",
    "def get_doc_list(doc_path):\n",
    "    querys = []\n",
    "    with open(doc_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            querys.append(preprocess(line))\n",
    "    return querys\n",
    "\n",
    "import pickle\n",
    "def load_object(file_path):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        object_instance = pickle.load(f)\n",
    "    return object_instance\n",
    "\n",
    "def saving_object(file_path, object_instance):\n",
    "    with open(file_path, 'wb') as f:\n",
    "        pickle.dump(object_instance, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "class PostingListConstructor(object):\n",
    "    \"\"\"docstring for PostingListConstructor\"\"\"\n",
    "    def __init__(self, docs_path):\n",
    "        super(PostingListConstructor, self).__init__()\n",
    "        self.docs_path = docs_path\n",
    "        self.posting_lists = {} # {term:(term_count, posting_list)}\n",
    "        self.num_docs_with_term = {}\n",
    "        self.len_docs = {} # {doc_id: _, doc_length: _}\n",
    "        self.doc_id_list = []\n",
    "        self.num_docs = 0\n",
    "        self.total_doc_length = 0\n",
    "        self.doc_wordcounters = []\n",
    "        self.original_lines = []\n",
    "        with open(docs_path, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "            check_next = 0\n",
    "            words = []\n",
    "            for i in range(len(lines)):\n",
    "                self.num_docs += 1\n",
    "                doc_id, text = lines[i].split('\\t')\n",
    "                terms = []\n",
    "                for token in tokenize(text):\n",
    "                    if token not in punctuation and token not in english_stop_words:\n",
    "                        terms.append(token)\n",
    "                        words.append(token)\n",
    "                        if (self.posting_lists.get(token, []) == []):\n",
    "                            self.posting_lists[token] = self.posting_lists.get(token, []) + [i]\n",
    "                        else:\n",
    "                            if self.posting_lists[token][-1] != i:\n",
    "                                self.posting_lists[token] = self.posting_lists.get(token, []) + [i]\n",
    "                        if (self.num_docs_with_term.get(token, 0) == 0):\n",
    "                            self.num_docs_with_term[token] = self.num_docs_with_term.get(token, 0) + 1\n",
    "                        else:\n",
    "                            if self.posting_lists[token][-1] != i:\n",
    "                                self.num_docs_with_term[token] = self.num_docs_with_term.get(token, 0) + 1\n",
    "                self.doc_id_list.append(doc_id)\n",
    "                self.doc_wordcounters.append(collections.Counter(terms))\n",
    "                self.original_lines.append(terms)\n",
    "                self.len_docs[i] = len(terms)\n",
    "                self.total_doc_length += len(terms)\n",
    "        self.terms = list(set(words))\n",
    "\n",
    "    def generate_td_list(self):\n",
    "        terms = self.terms\n",
    "        td_idf_list = []\n",
    "        for term in terms:\n",
    "            pointers = self.posting_lists[term]\n",
    "            doc_freq = self.num_docs_with_term[term]\n",
    "            idf = math.log(float(self.num_docs)/float(doc_freq))\n",
    "            td_sub_list = []\n",
    "            idf_list = []\n",
    "            for pointer in pointers:\n",
    "                td_sub_list.append((pointer, self.doc_wordcounters[pointer][term]*idf))\n",
    "            td_idf_list.append((term, td_sub_list))\n",
    "        td_idf = dict(td_idf_list)\n",
    "        return td_idf\n",
    "\n",
    "posting_list_generator = PostingListConstructor('./dev.docs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sweet', 'chocolate'] 935 ['chocolate', 'chocolate', 'processed', 'typically', 'sweetened', 'food', 'produced', 'seed', 'tropical', 'theobroma', 'cacao', 'tree', 'cacao', 'cultivated', 'least', 'three', 'millennia', 'mexico', 'central', 'america', 'northern', 'south', 'america', 'earliest', 'documented', 'use', 'around', '1100', 'bc', 'majority', 'mesoamerican', 'people', 'made', 'chocolate', 'beverages', 'including', 'aztecs', 'made', 'beverage', 'known', 'xocol\\\\u0101tl', 'nahuatl', 'word', 'meaning', 'bitter', 'water', 'seeds', 'cacao', 'tree', 'intense', 'bitter', 'taste', 'must', 'fermented', 'develop', 'flavor', 'fermentation', 'beans', 'dried', 'cleaned', 'roasted', 'shell', 'removed', 'produce', 'cacao', 'nibs', 'nibs', 'ground', 'cocoa', 'mass', 'pure', 'chocolate', 'rough', 'form', 'cocoa', 'mass', 'usually', 'liquefied', 'molded', 'without', 'ingredients', 'called', 'chocolate', 'liquor', 'liquor', 'also', 'may', 'processed', 'two', 'components', 'cocoa', 'solids', 'cocoa', 'butter', 'unsweetened', 'baking', 'chocolate', 'bitter', 'chocolate', 'contains', 'primarily', 'cocoa', 'solids', 'cocoa', 'butter', 'varying', 'proportions', 'much', 'chocolate', 'consumed', 'today', 'form', 'sweet', 'chocolate', 'combining', 'cocoa', 'solids', 'cocoa', 'butter', 'fat', 'sugar', 'milk', 'chocolate', 'sweet', 'chocolate', 'additionally', 'contains', 'milk', 'powder', 'condensed', 'milk', 'white', 'chocolate', 'contains', 'cocoa', 'butter', 'sugar', 'milk', 'cocoa', 'solids', 'cocoa', 'solids', 'contain', 'alkaloids', 'theobromine', 'phenethylamine', 'caffeine', 'physiological', 'effects', 'body', 'linked', 'serotonin', 'levels', 'brain', 'research', 'found', 'chocolate', 'eaten', 'moderation', 'lower', 'blood', 'pressure', 'presence', 'theobromine', 'renders', 'chocolate', 'toxic', 'animals', 'especially', 'dogs', 'cats', 'chocolate', 'become', 'one', 'popular', 'food', 'types', 'flavors', 'world', 'vast', 'number', 'foodstuffs', 'involving', 'chocolate', 'created', 'chocolate', 'chip', 'cookies', 'become', 'common', 'popular', 'parts', 'europe', 'north', 'america', 'gifts', 'chocolate', 'molded', 'different', 'shapes', 'become', 'traditional', 'certain', 'holidays', 'chocolate', 'also', 'used', 'cold', 'hot', 'beverages', 'produce', 'chocolate', 'milk', 'hot', 'chocolate', 'cocoa', 'mass', 'used', 'originally', 'mesoamerica', 'beverage', 'ingredient', 'foods', 'chocolate', 'played', 'special', 'role']\n"
     ]
    }
   ],
   "source": [
    "class BM25(object):\n",
    "\n",
    "    def __init__(self, corpus_constructor, k1 = 1.5, b= 0.75):\n",
    "        self.corpus_size = corpus_constructor.num_docs\n",
    "        self.avgdl = float(sum(corpus_constructor.len_docs.values())) / self.corpus_size\n",
    "        self.posting_list_generator = corpus_constructor\n",
    "        self.f = []\n",
    "        self.df = {}\n",
    "        self.idf = {}\n",
    "        self.PARAM_K1 = k1\n",
    "        self.PARAM_B = b\n",
    "    def get_sent_score(self, querys):\n",
    "        idf = []\n",
    "        document_index = []\n",
    "        document_scores = {}\n",
    "        PARAM_K1 = self.PARAM_K1\n",
    "        PARAM_B  = self.PARAM_B\n",
    "        for query in querys:\n",
    "            pointers = self.posting_list_generator.posting_lists.get(query, 0)\n",
    "            if pointers != 0:\n",
    "                document_index.extend(pointers)\n",
    "        document_index = list(set(document_index))\n",
    "        for i in document_index:\n",
    "            scoredq = 0\n",
    "            for query in querys:\n",
    "                score = 0\n",
    "                n = self.posting_list_generator.num_docs\n",
    "                nqi = self.posting_list_generator.num_docs_with_term.get(query, 0)\n",
    "                idfi = math.log((n-nqi+0.5) /(nqi+0.5))\n",
    "                fqid = self.posting_list_generator.doc_wordcounters[i].get(query, 0)\n",
    "                absd = self.posting_list_generator.len_docs[i]\n",
    "                tfi = (fqid*(PARAM_K1+1))/(fqid+PARAM_K1*(1-PARAM_B+PARAM_B*absd/self.avgdl))\n",
    "                scorei = idfi*tfi\n",
    "                scoredq += scorei\n",
    "            document_scores[i] = scoredq\n",
    "        return document_scores\n",
    "\n",
    "def get_sent_querys_dict(query_path, bm25model):\n",
    "    querylist = []\n",
    "    querys = get_doc_list(query_path)\n",
    "    for query in querys:\n",
    "        querylist.append(bm25model.get_sent_score(query))\n",
    "    return querylist\n",
    "\n",
    "import operator\n",
    "\n",
    "bm25model = BM25(posting_list_generator)\n",
    "querys = get_doc_list('./test_query')\n",
    "querytest = get_sent_querys_dict('./test_query', bm25model)\n",
    "\n",
    "for i in range(len(querytest)):\n",
    "    max_index = max(querytest[i].iteritems(), key=operator.itemgetter(1))[0]\n",
    "    print querys[i], max_index, posting_list_generator.original_lines[max_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "0ed13adeb998da4bc552767f0b7d27fdc203d61647f4f5275191afdf"
   },
   "source": [
    "```The tfidf and bm25 algorithm are implemented according to the lecture material. I also use some machanism to remove the stop words and punctuation from nltk. I chose some typical types of query and test file to test how well the tfidf and bm25 works. The outcomes show that for the term or query which are highly relevant to a document, the tfidf and bm25 scores will be accordingly high. ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "9be733fd694ae2d84d9be526802474be241002b0ed7cc8324dfed9db"
   },
   "source": [
    "## Translating the queries (40% of assignment mark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "signature": "d131166852abe212c8c851dcc7769348e460d873cc63efcfa9dba158"
   },
   "source": [
    "For translation, you should implement a simple word-based translation model in a noisy channel setting, where you search for the best translation string using a language model over the English with a translation model over the back-translation of the English output string, $\\vec{e}$, into the German input string, $\\vec{f}$. This corresponds to finding the string, $\\vec{e}$ which maximises $p(\\vec{e}) p(\\vec{f} | \\vec{e})$. This has two components:\n",
    "\n",
    "### Language model\n",
    "\n",
    "The first step is to estimate a language model. You should learn both a unigram language model and a trigram language model, and compare the two. Note that the unigram will be used in the *decoding* step below. You will have to think about how to smooth your estimates, and the related problem of handling unknown words. For smoothing the trigram model, you should you use backoff smoothing. There are a few choices to be made about how much probability mass to assign to the backoff, you will need to decide how to do this -- see the lecture slides and readings for some ideas. Be careful to pad the sentence with sentinel symbols to denote the start and end of each sentence such that the model can predict the words at the start of a sentence, and those when a sentence is complete.\n",
    "\n",
    "Please use *bitext-large.en* to train your models (start with *bitext-small.en* for your development) and evaluate the perplexity of your model on *newstest2013.en*. You should justify your development decisions and parameter settings in a markdown cell, and quantify their effect on perplexity, including the differences you notice between the unigram and the trigram models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "signature": "0e114d8958fa3f3f95f3211a7bc58a86a620b40d97a10012762114bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', '<s>', 'president', '</s>', '</s>']\n",
      "0.517627693901\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import string\n",
    "import collections\n",
    "import math\n",
    "from nltk.tokenize import word_tokenize\n",
    "from six import iteritems\n",
    "from six.moves import xrange\n",
    "\n",
    "punctuation = set(string.punctuation)\n",
    "english_stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "german_stop_words = set(nltk.corpus.stopwords.words('german'))\n",
    "\n",
    "porter_stemmer = nltk.stem.porter.PorterStemmer()\n",
    "\n",
    "\n",
    "def tokenize(line, tokenizer = word_tokenize):\n",
    "    utf_line = line.decode('utf-8').lower()\n",
    "    return [token.encode('ascii', 'backslashreplace') for token in tokenizer(utf_line)]\n",
    "\n",
    "def preprocess(doc):\n",
    "    terms = []\n",
    "    for token in tokenize(doc):\n",
    "        if token not in punctuation and token not in english_stop_words:\n",
    "            terms.append(token)\n",
    "    return terms\n",
    "\n",
    "def preprocess_de(doc):\n",
    "    terms = []\n",
    "    for token in tokenize(doc):\n",
    "        if token not in punctuation and token not in german_stop_words:\n",
    "            terms.append(token)\n",
    "    return terms\n",
    "\n",
    "def get_doc_list(doc_path):\n",
    "    querys = []\n",
    "    with open(doc_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            querys.append(preprocess(line))\n",
    "    return querys\n",
    "\n",
    "#this trigram with backoff smoothing\n",
    "#with small bugs\n",
    "from nltk.util import ngrams\n",
    "PARAM_K = 0\n",
    "def get_en_list(doc_path):\n",
    "    querys = []\n",
    "    with open(doc_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            querys.append(preprocess(line))\n",
    "    return querys\n",
    "\n",
    "def get_de_list(doc_path):\n",
    "    querys = []\n",
    "    with open(doc_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            querys.append(preprocess_de(line))\n",
    "    return querys\n",
    "\n",
    "#original list of list of token, after punctuation and stop word removal\n",
    "origianl_docs = get_en_list('./bitext-large.en')\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "def check_for_unk_train(word,unigram_counts):\n",
    "    if word in unigram_counts:\n",
    "        return word\n",
    "    else:\n",
    "        unigram_counts[word] = 0\n",
    "        return \"UNK\"\n",
    "\n",
    "def convert_sentence_train(sentence,unigram_counts):\n",
    "    return [\"<s>\"] + [check_for_unk_train(token.lower(),unigram_counts) for token in sentence] + [\"</s>\"]\n",
    "\n",
    "def convert_tri_sentence_train(sentence,unigram_counts):\n",
    "    return [\"<s>\",\"<s>\"] + [check_for_unk_train(token.lower(),unigram_counts) for token in sentence] + [\"</s>\", \"</s>\"]\n",
    "\n",
    "def get_bigram_counts(sentences):\n",
    "    bigram_counts = defaultdict(dict)\n",
    "    unigram_counts = {}\n",
    "    for sentence in sentences:\n",
    "        sentence = convert_sentence_train(sentence, unigram_counts)\n",
    "        for i in range(len(sentence) - 1):\n",
    "            bigram_counts[sentence[i]][sentence[i+1]] = bigram_counts[sentence[i]].get(sentence[i+1],0) + 1\n",
    "            unigram_counts[sentence[i]] = unigram_counts.get(sentence[i],0) + 1\n",
    "    token_count = float(sum(unigram_counts.values()))\n",
    "    unigram_counts[\"</s>\"] = unigram_counts[\"<s>\"]\n",
    "    return unigram_counts, bigram_counts, token_count\n",
    "\n",
    "def get_trigram_counts(sentences):\n",
    "    trigram_counts = defaultdict(dict)\n",
    "    unigram_counts = {}\n",
    "    for sentence in sentences:\n",
    "        sentence = convert_tri_sentence_train(sentence, unigram_counts)\n",
    "        for i in range(len(sentence) - 2):\n",
    "            trigram_counts[(sentence[i], sentence[i+1])][sentence[i+2]] = trigram_counts[(sentence[i], sentence[i+1])].get(sentence[i+2],0) + 1\n",
    "            unigram_counts[sentence[i]] = unigram_counts.get(sentence[i],0) + 1\n",
    "    token_count = float(sum(unigram_counts.values()))\n",
    "    unigram_counts[\"</s>\"] = unigram_counts[\"<s>\"]\n",
    "    return unigram_counts, trigram_counts, token_count\n",
    "\n",
    "from numpy.random import choice \n",
    "\n",
    "def generate_sentence(bigram_counts):\n",
    "    sentence = [\"<s>\"]\n",
    "    while sentence[-1] != \"</s>\":\n",
    "        prev_token_counts = bigram_counts[sentence[-1]]\n",
    "        bigram_probabilities = []\n",
    "        total_counts = float(sum(prev_token_counts.values()))\n",
    "        for token in prev_token_counts:\n",
    "            bigram_probabilities.append(prev_token_counts[token]/total_counts)\n",
    "        sentence.append(choice(prev_token_counts.keys(), p=bigram_probabilities))\n",
    "    return \" \".join([sentence[1].title()] + sentence[2:-1]).replace(\" ,\",\",\").replace(\" .\", \".\")\n",
    "\n",
    "docs_unigrams, docs_trigrams, docs_token_count = get_trigram_counts(origianl_docs)\n",
    "docs_unigrams1, docs_bigrams, docs_token_count1 = get_bigram_counts(origianl_docs)\n",
    "docs_uni_value_counts = collections.Counter(docs_unigrams1.values())\n",
    "\n",
    "import math\n",
    "\n",
    "def get_log_prob_interp(sentence,i, unigram_counts,bigram_counts, trigram_counts, token_count, bigram_lambda, trigram_lambda):\n",
    "    tri_full_hit = trigram_counts.get((sentence[i-2],sentence[i-1]), 0)\n",
    "    if tri_full_hit != 0:\n",
    "        tri_full_hit = tri_full_hit.get(sentence[i], 0)\n",
    "    return math.log(trigram_lambda*tri_full_hit/float(bigram_counts[sentence[i-2]].get(sentence[i-1], 1)) + \n",
    "        (1 - trigram_lambda) *\n",
    "        (bigram_lambda*bigram_counts[sentence[i-1]].get(sentence[i],0)/float(unigram_counts[sentence[i-1]]) + \n",
    "                    (1 - bigram_lambda)*unigram_counts[sentence[i]]/token_count))\n",
    "\n",
    "def check_for_unk_test(word,unigram_counts):\n",
    "    if word in unigram_counts and unigram_counts[word] > 0:\n",
    "        return word\n",
    "    else:\n",
    "        return \"UNK\"\n",
    "\n",
    "def convert_sentence_test(sentence,unigram_counts):\n",
    "    return [\"<s>\", \"<s>\"] + [check_for_unk_test(word.lower(),unigram_counts) for word in sentence] + [\"</s>\", \"</s>\"]\n",
    "\n",
    "def convert_sentence_uni_test(sentence,unigram_counts):\n",
    "    return [\"<s>\"] + [check_for_unk_test(word.lower(),unigram_counts) for word in sentence] + [\"</s>\"]\n",
    "\n",
    "def get_sent_log_prob_interp(sentence, unigram_counts, bigram_counts, trigram_counts, token_count, bigram_lambda, trigram_lambda):\n",
    "    sentence = convert_sentence_test(sentence, bigram_counts)\n",
    "    return sum([get_log_prob_interp(sentence,i, unigram_counts,bigram_counts, trigram_counts, token_count, bigram_lambda, trigram_lambda) for i in range(2,len(sentence))])\n",
    "\n",
    "sentence = \"president\".split()\n",
    "print convert_sentence_test(sentence, docs_bigrams)\n",
    "print get_sent_log_prob_interp(sentence, docs_unigrams1, docs_bigrams, docs_trigrams, docs_token_count1, 0.95, 0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "interpolation\n",
      "0.99 0.99\n",
      "113896.479707\n",
      "0.99 0.95\n",
      "32778.0248501\n",
      "0.99 0.75\n",
      "9756.73645335\n",
      "0.99 0.5\n",
      "6101.38281902\n",
      "0.99 0.25\n",
      "5005.81145674\n",
      "0.99 0.001\n",
      "8864.36562582\n",
      "0.95 0.99\n",
      "53786.4347866\n",
      "0.95 0.95\n",
      "15479.1630025\n",
      "0.95 0.75\n",
      "4607.57513867\n",
      "0.95 0.5\n",
      "2881.06821197\n",
      "0.95 0.25\n",
      "2362.38170456\n",
      "0.95 0.001\n",
      "3835.95939287\n",
      "0.75 0.99\n",
      "25997.6367834\n",
      "0.75 0.95\n",
      "7482.06220759\n",
      "0.75 0.75\n",
      "2227.30720943\n",
      "0.75 0.5\n",
      "1392.2200256\n",
      "0.75 0.25\n",
      "1138.82950407\n",
      "0.75 0.001\n",
      "1669.6583844\n",
      "0.5 0.99\n",
      "19797.9548929\n",
      "0.5 0.95\n",
      "5698.05484495\n",
      "0.5 0.75\n",
      "1696.56678969\n",
      "0.5 0.5\n",
      "1060.3655046\n",
      "0.5 0.25\n",
      "865.731940879\n",
      "0.5 0.001\n",
      "1223.09680385\n",
      "0.25 0.99\n",
      "17883.5967627\n",
      "0.25 0.95\n",
      "5147.33979207\n",
      "0.25 0.75\n",
      "1533.07905097\n",
      "0.25 0.5\n",
      "958.52023226\n",
      "0.25 0.25\n",
      "782.245196532\n",
      "0.25 0.001\n",
      "1092.80910284\n",
      "0.001 0.99\n",
      "21570.9974026\n",
      "0.001 0.95\n",
      "6209.01113357\n",
      "0.001 0.75\n",
      "1850.12284675\n",
      "0.001 0.5\n",
      "1157.85648081\n",
      "0.001 0.25\n",
      "946.733280036\n",
      "0.001 0.001\n",
      "1371.8881601\n"
     ]
    }
   ],
   "source": [
    "def calculate_perplexity(sentences,unigram_counts,bigram_counts, trigram_counts, token_count, smoothing_function, parameter1, parameter2):\n",
    "    total_log_prob = 0\n",
    "    test_token_count = 0\n",
    "    for sentence in sentences:\n",
    "        test_token_count += len(sentence) + 2 # have to consider the end token\n",
    "        total_log_prob += smoothing_function(sentence,unigram_counts,bigram_counts, trigram_counts, token_count, parameter1, parameter2)\n",
    "    return math.exp(-total_log_prob/test_token_count)\n",
    "\n",
    "test_set = get_en_list('./newstest2013.en')\n",
    "\n",
    "print \"interpolation\"\n",
    "for bigram_lambda in [0.99,0.95,0.75,0.5,0.25,0.001]:\n",
    "    for trigram_lambda in [0.99,0.95,0.75,0.5,0.25,0.001]:\n",
    "        print bigram_lambda, trigram_lambda\n",
    "        print calculate_perplexity(test_set,docs_unigrams,docs_bigrams,docs_trigrams, docs_token_count,get_sent_log_prob_interp,bigram_lambda, trigram_lambda)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PARAM_K is the threshold of counts. For a particular gram, if C is greater than k, the backoff machanism will not be applied. If the count is less than this threshold, which means the occurrence of this gram is not much enough, so the backoff machanism will be applied, and a lower ngram machinsm will measure the probability of the few-occurrece grams. For the poor time complexity of my ngram model, I failed to evaluate the perplexity in the test set. However, I simplify the test set, and choose randomly 1000 lines from the test set. Due to the poor quality of the implementation of backoff, I decided to change the practical smoothing machanism of interpolation.\n",
    "\n",
    "In the interpolation machanism, there are a parameter lambda. For trigram language model, there are 2 lambdas for trigram and bigram respectively.Shown as the outcomes above when the lambdas are both high, the perplexity is extremely high. It goes down with the decreas of the 2 lambdas. By doing the test, when the system get the lowest perplexity, the 2 lambdas were both 0.25. So in the next experiment, I decided to use these two figures.\n",
    "\n",
    "For this part, only trigram language model has been implemented, and evaluated. For the unigram language model with good turing smoothing, I will evaluate it at the conclusive model of this module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "f8cbf7aca8b8d53f908e5ff0c869f562a2d52c4a3fe32400c131ddc9"
   },
   "source": [
    "### Translation model\n",
    "\n",
    "The next step is to estimate translation model probabilities. For this we will use a word-based alignment model, e.g., *ibm1* to learn word-based translation probabilities using expectation maxisation. Your task is to obtain good quality word alignments from this data, which will require careful use of the word alignment models. You will want to training in both translation directions, and combining the alignments.\n",
    "\n",
    "You should use the *bitext-small* files for this purpose, and be aware that it may take a minute or two to train ibm1 on this data. Inspect some of the word alignments and translation probabilities (e.g., using a few common words in German such as *haus*) to see if your approach is working, and give some examples of output alignments that you find that are good and bad. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "signature": "33d81e9091a757fb440c60e65a0d8178361996bada694af7ea6a3215"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate import IBMModel1\n",
    "from nltk.translate import AlignedSent, Alignment\n",
    "\n",
    "def get_en_list(doc_path):\n",
    "    querys = []\n",
    "    with open(doc_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            querys.append(preprocess(line))\n",
    "    return querys\n",
    "\n",
    "def get_de_list(doc_path):\n",
    "    querys = []\n",
    "    with open(doc_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            querys.append(preprocess_de(line))\n",
    "    return querys\n",
    "\n",
    "\n",
    "en_text = get_en_list('./bitext-small.en')\n",
    "de_text = get_de_list('./bitext-small.de')\n",
    "\n",
    "all_en_words_set = set([item for sublist in en_text for item in sublist])\n",
    "all_de_words_set = set([item for sublist in de_text for item in sublist])\n",
    "\n",
    "all_en_words = list(all_en_words_set)\n",
    "all_de_words = list(all_de_words_set)\n",
    "\n",
    "#english to german\n",
    "bitext_en_de = zip(en_text, de_text)\n",
    "bt_en_de = [AlignedSent(E,F) for E,F in bitext_en_de]\n",
    "imb1_en_de = IBMModel1(bt_en_de, 5)\n",
    "tm_en_de = imb1_en_de.translation_table\n",
    "\n",
    "#english to german\n",
    "bitext_de_en = zip(de_text, en_text)\n",
    "bt_de_en = [AlignedSent(E,F) for E,F in bitext_de_en]\n",
    "imb1_de_en = IBMModel1(bt_de_en, 5)\n",
    "tm_de_en = imb1_de_en.translation_table\n",
    "\n",
    "print 'finished'\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<function <lambda> at 0x000000011063E048>, {'chile': 0.00015166070253387994, 'business\\\\u201c': 0.0032871329181235367, 'b\\\\xfcndnisgrenzen': 0.10944188675423058, 'sieht': 5.6998985693486826e-05, 'grenzgebiet': 0.09707683355773049, '\\\\xf6l': 2.909611575055009e-06, 'unsicherheit': 9.576339660187786e-07, '\\\\u201ethe': 0.0036946244498101007, 'jahre': 2.4354254190197598e-08, 'entsprechend': 0.0004402240357246591, 'entdeckt': 0.00010177297677489431, 'bosnien': 1.8974044148007754e-05, 'letzten': 2.1904111696832166e-10, 'w\\\\xfcrden': 3.9746293100444196e-09, 'regierung': 1.4723692786175813e-11, 'wandels': 9.486447907150684e-06, 'au\\\\xdferhalb': 1.2582370601942222e-06, 'zeit': 5.773527511412098e-11, 'noten': 0.035044730588485666, 'bulgarien': 2.7603686530199758e-05, 'einziehen': 0.04129895213923631, '198': 0.019511727777349327, 'schlug': 0.005268808462240733, 'gerste': 0.06059668585656223, 'nieder': 0.0021310809484891813, 'pal\\\\xe4stinenser': 2.560950662439944e-05, 'entwicklung': 1.957300417112993e-09, 'f\\\\xfchlen': 0.00010536041354804499, 'hinaus': 3.5671516785232432e-09, 'fortschritte': 6.940685416173932e-06, 'gr\\\\xf6\\\\xdften': 6.049491801466465e-07, 'bezahlen': 1.1690983242076288e-05, 'lange': 1.1516067355838863e-09, 'drei': 2.558223319457161e-11, 'kontrollbereichs': 0.019511727777349327, 'vorher': 0.00019291374385334826, 'einrichtung': 0.0035536014582256323, 'infolge': 2.3664481694602628e-05, 'eroberte': 0.12635754533921698, 'norden': 2.9324459460843153e-07, '\\\\u2013': 1.1945098890637512e-11, 'chongqing': 0.17926235865960546, 'h\\\\xf6rendes': 0.10944188675423058, 'grenze': 7.649421603652446e-09, 'pflanzenarten': 0.13505095329858016, 'brutal': 0.001576166608248942, '34': 0.032943659002621625, 'umgeknickt': 0.01684296612121291, 'schlie\\\\xdft': 0.00048258858592169804, 'bilden': 7.806016462625396e-05, '1908': 0.01684296612121291, 'gr\\\\xf6\\\\xdferer': 0.0004844081617922567, 'noyau': 0.13841628580000653, 'klares': 0.000883962578003496, 'tempelberg-bezirk': 0.10715206248571314, 'h\\\\xe4ngt': 1.4779551966668752e-05, 'ariel': 0.10715206248571314, 'mitglied': 8.38633977135525e-06, 'ballungsgebiet': 0.17926235865960546, 'staaten': 1.0157352041433348e-11, 'gebiete': 2.2555802947448582e-05, 'besuch': 7.449349090916009e-06, 'arme': 1.192494165257918e-05, 'distrikt': 0.04599530461211544, 'zur\\\\xfccklegen': 0.01320899229688304, 'b\\\\xe4ume': 9.219063800343847e-05, 'israel': 5.380906279854075e-08, 'landes': 2.2239454253717384e-09, 'osten': 2.0786376344485502e-10, 'hilfreich': 0.001194440945905554, 'warnzeichen': 0.02689530889742682, 'schweiz': 1.2644960260532827e-06, 'verlieren': 3.167626874862426e-07, 'l\\\\xe4nder': 2.0116002767669877e-09, 'detonierte': 0.01684296612121291, 's\\\\xfcden': 2.7609988077968936e-07, 'weiterer': 0.02299371136852565, 'kleinerer': 0.004585350939460578, 'idealerweise': 0.0008254018038348412, 'sicherheit': 1.718782248561346e-07, 'weiteren': 2.892479478226576e-06, 'beantworten': 0.0009692648700228041, 'dl': 0.019511727777349327, 'tag': 6.2215019932557254e-09, 'mehr': 5.920872206451971e-06, 'weiterhin': 1.7456660038298346e-05, 'gebietes': 0.37799031896692226, 'kenia': 6.861179393119866e-06, 'island': 0.00012006158422988648, 'geschrumpft': 0.0034207078290789832, 'sch\\\\xfcler': 1.7746158920009367e-05, 'km2': 0.01684296612121291, 'integrierter': 0.028415093913105115, 'objekt': 0.00033811850559820836, 'zugang': 0.0002909329929761309, 'bedeutung': 1.2137846269562532e-05, 'widerst\\\\xe4nde': 0.02702017546488857, 'bereichen': 7.518146289021971e-05, 'angezogen': 0.047933193120469286, 'gr\\\\xf6\\\\xdfe': 2.7806244512341036e-05, 'gedeihen': 0.0647031808325369, 'westen': 5.650092001446507e-10, 'simbabwische': 0.048843258415479346, 'bujumbura': 0.010789910360819522, 'japan': 1.6829424316495493e-08, 'insgesamt': 1.9431179552919605e-05, '27': 0.004482262005980813, '20': 1.5953486154685905e-08, 'f\\\\xfcr': 1.4977507577261563e-07, 'waldgebiete': 0.28837724267877063, 'blauer': 0.024522975756674545, 'staat': 3.988602590100133e-08, 'umfasst': 0.00021370374921452065, 'produktion': 1.7262249436895192e-06, 'unabh\\\\xe4ngigen': 0.0427262178300763, 'band': 0.00017976727836794523, 'spielfeld': 0.004850229850067751, 'gestiegen': 0.00014835507806003418, '\\\\u201egro\\\\xdfen': 0.003055226364193698, 'wiederaufbaus': 0.027709067344797365, 'kern': 2.4471309435316444e-05, 'kommt': 1.5814361939376393e-06, 'tats\\\\xe4chlich': 7.166252570645171e-10, 'entscheidend': 2.584148083689854e-05, 'medien': 0.00018752249087331037, 'freihandelszone': 0.2045708508124526, 'ausdehnen': 0.01626721069278555, 'seen\\\\u201c': 0.027709067344797365, 'countdown\\\\u201c': 0.029910804125339165, 'eu': 7.787312589605301e-06, '80': 3.161825006371728e-07, 'out': 0.04667651787258969, 'herausforderungen': 3.5698702595280167e-06, 'bereich': 0.4803138413583493, 'ungef\\\\xe4hr': 1.2489104711986665e-06, 'bald': 1.9751294848395564e-07, 'viermal': 0.004418698143919962, 'mohammedanische': 0.12635754533921698, 'geh\\\\xf6re': 0.23349255046226056, 'brauchen': 3.7490728114321283e-07, 'weniger': 7.409216399403855e-10, 'gemeinsamen': 4.62186981385853e-05, 'st\\\\xe4rken': 6.747369657796128e-05, 'dortigen': 0.0023086289204099606, 'weltweiten': 1.3404700760133662e-06, 'norwegen': 1.7797987103567788e-05, 'frieden': 7.789902821011078e-08, 'deshalb': 0.00025433922523645743, 'verwendung': 1.227702035097192e-05, 'vorschlag': 7.065114872030123e-07, '\\\\xfcberraschen': 0.004186712510507564, 'provozierenden': 0.10715206248571314, 'abzuschlie\\\\xdfen': 0.02711016620427482, 'starkes': 4.6699142932827345e-05, 'eu-freihandelszone': 0.16627806969608513, 'sharon': 0.10715206248571314, 'folge': 2.746883624134627e-05, 'millionen': 1.2321429684844213e-08, 'engagement': 1.6454053085759095e-06, 'system': 3.4624374299075897e-09, 'euro': 8.277935824617058e-08, 'final': 0.029910804125339165, 'gesamte': 2.805981359543741e-06, 'milit\\\\xe4r': 3.1023439838423475e-06, 'montenegro': 0.00031391140227433445, 'provinz': 1.2844548576577539e-06, 'w\\\\xfcrde': 1.502117529389134e-10, 'sicherlich': 1.1246071876187233e-05, '10': 2.148116637181241e-06, '15': 1.1675229209327388e-07, '17': 6.439554382813859e-06, '16': 0.000303494687136498, None: 5.143393361195575e-07, 'neben': 5.920489384454847e-05, 'stehenden': 0.0011797096651871078, 'riesiges': 0.00040378083637712485, 'mitte': 4.242438199967558e-07, 'dass': 1.664784350849742e-08, 'siachen': 0.019511727777349327, 'serbien': 8.140645795585929e-07, 'entscheidendes': 0.004867017424578586, 'kroatien': 0.0005556092665536914, 'riesigen': 9.596437230719636e-05, 'jahr': 2.9336273847814557e-08, 'b\\\\xfcrgern': 0.0003969692018069582, 'kaukasus': 2.621197951024187e-05, 'drittens': 5.114478949551787e-05, 'magisch': 0.20359292681669516, 'zufriedenstellendes': 0.09707683355773049, 'charakteristisch': 0.10612602297016994, 'soweit': 6.0517643211092404e-05, '778': 0.019511727777349327, 'republikanisches': 0.024522975756674545, 'plan': 5.050759373486148e-07, 'bauern': 1.1582695643865576e-06, 'schule': 9.020718274256671e-08, 'definiert': 1.505535309271967e-05, 'kleine': 1.647261730329355e-08, 'km': 0.013699851025173243, 'liechtenstein': 0.15277846345500115, 'erfolg': 2.0902814163972414e-09, 'unserer': 0.00025835213387218606, 'kosovo': 1.288858454654399e-07, 'st\\\\xe4dtischen': 0.00018576824941863882, 'verliert': 0.001262285621416499, 'kontrolle': 1.2580705946797698e-08, 'h\\\\xe4ufig': 3.863197400201612e-10, 'gestellt': 3.2391081844094838e-06, 'bildung': 2.829807541776467e-07, 'gewaltiges': 0.0009122195189624577, 'daher': 3.869748211145621e-07, 'sp\\\\xe4ter': 1.048936487486236e-07, 'konfrontationen': 0.0676056040052131, 'sehen': 4.4043460830326444e-07, 'darstellt': 0.0003492826964415353, 'hit': 0.029910804125339165, 'de': 6.229336295376534e-09, 'exporte': 0.00019736872033442252, 'wolga-ural-gebiet': 0.12635754533921698, 'leben': 3.56387092193742e-09, 'immer': 5.41374958629577e-10, 'einzelner': 0.00035131315402378956, 'zeigen': 1.2333069914328908e-05, 'benutzen': 0.0012427501987026784, 'geld': 5.572217619003269e-10, 'andenregion': 0.13505095329858016, 'betr\\\\xe4chtlich': 0.03323118422274451, 'w\\\\xe4hrend': 9.22129287649523e-07, 'pr\\\\xe4zedenzfall': 0.002468286406048053, 'einfach': 1.846855749828171e-10, 'co2': 1.2809291487066987e-05, 'k\\\\xf6nnte': 4.765442076605814e-09, 'wirtschaft': 1.0366239291869864e-07, 'atombombe': 0.00033593728098503436, 'finger': 0.0474178139165602, 'offensichtlich': 2.8993566754429557e-06, 'europe': 0.003165102696517346, 'einheitsw\\\\xe4hrung': 0.004319718410592081, 'seiten': 6.721828613848512e-07, 'leitungen': 0.10530582876397525, 'hektar': 1.8810776568150504e-05, 'problem': 1.1004144150652372e-08, 'waldfl\\\\xe4che': 0.14214437574504588, 'c': 0.0010568369961446423, 'darin': 0.00011262566288950207, 'region': 3.0018269939423802e-06, 'derartiger': 0.02297801027384471, 'hauptstadt': 2.1800703737829418e-07, 'uno': 4.480908185005447e-05, 'wald': 4.1312036764498266e-05, 'aktuell': 0.000374844064318187, 'abgedeckt': 0.024684396038883186, 'ca': 0.0009229516906922014, 'erleben': 4.795808747963516e-06, 'japans': 8.516380434148935e-06, 'ebenes': 0.04007856297647692, 'st\\\\xe4rke': 2.5730787468981876e-06, 'union': 7.638779766699226e-09, 'geh\\\\xf6ren': 2.3246762033134633e-05, 'wichtige': 1.8422832098120728e-06, 'dur': 0.09664601076757522, 'eurozone': 5.6278909246619415e-05, 'wurde': 4.011246637935664e-06, 'zweitgr\\\\xf6\\\\xdften': 0.045970909650295065, 'verlagert': 0.00018997451691593324, 'heute': 7.763455808223541e-12, 'gewaltt\\\\xe4tigste': 0.027709067344797365, 'bereiten': 0.00286004319105809, 'vage': 0.10855731887213066, 'dieforces': 0.010789910360819522, 'nigerianischen': 0.010778741016663299, 'erstreckt': 1.0326002532577198e-05, 'umwandelt': 0.09012601357824572, 'durchschnitt': 0.00033091973705967436, 'lib\\\\xe9ration': 0.010789910360819522, 'gesteigert': 0.24193476678779322, 'landwirtschaftlichen': 0.0002358430768130847, 'ziehen': 0.00012148466678906207, 'nationales': 0.013360460356340384, 'grenz\\\\xfcbereinkommen': 0.0676056040052131, 'gl\\\\xfccklicherweise': 0.00014742375511891023, 'nationen': 1.381481535543207e-06, 'sowie': 5.888656338461836e-07, 'steht': 4.5246445272952403e-07, 'bogen': 0.0033814087595674916, 'roland': 0.035044730588485666, 'ewr': 0.15277846345500115, 'schwedischen': 5.5729662320153116e-05, 'fl\\\\xe4che': 0.47546897213017514, '\\\\xfcbernommen': 0.00022342118534904776, 'umstrittenes': 0.023018199697149345, 'verboten': 0.07224265152731765, '\\\\xf6stlichen': 0.00032855778382208985, 'umspannt': 0.0004191845287968542, 'w\\\\xe4ren': 3.01340544442978e-06, 'zentraler': 0.0004937913666035798, 'solide': 0.009648790235434059, 'albanien': 0.0003896633672453904, 'libanesische': 0.022212158268977128, 'europ\\\\xe4ischen': 9.497095494714835e-09, 'welt': 3.717453395778681e-10, '14,6': 0.0019498896045224703, 'feststellbar': 0.001843033450295336, 'marktwirtschaftlichen': 0.012769226349491436, 'eigenen': 0.0003425799800121926, 'nachdem': 7.521482460560662e-05, 'passender': 0.029910804125339165, 'lese': 0.14214437574504588, 'einwohnern': 2.2811062280213073e-06, 'gebiet': 0.831033092029074, 'waffenstillstand': 0.0033682536490502344, 'missbrauch': 0.0022724836401574423, 'euroraum': 0.3322567701339952, 'w\\\\xe4hrungsraums': 0.35928795271545744, 'mitgliedern': 0.00041302252995190184, 'frage': 2.3134918007801265e-08, 'verwalten': 0.0011264442396625145, 'nato': 3.228329094376098e-10, '4': 8.045018648784969e-07, 'gute': 5.683425412486835e-09, 'k\\\\xf6nnen': 3.608839227863477e-08, 'k\\\\xe4mpft': 0.0003396538434183354, 'neuen': 1.390452971041047e-09, 'zentralasien': 1.6517244556562037e-06, 'w\\\\xe4re': 3.351400153862827e-08, 'l\\\\xe4ndern': 1.235379275842973e-10, 'fryer': 0.035044730588485666, 'wobei': 0.00011945231786693093, 'beschr\\\\xe4nkt': 5.784297791329828e-07, 'mussten': 0.0002627215303648562, 'abkommen': 2.8939083452307326e-05, 'nachbarn': 8.909225595372268e-06, 'teil': 2.119253993231719e-10, 'jedoch': 3.4329724701560836e-09, 'wohlstand': 6.036840158172832e-06, 'h\\\\xe4ndler': 0.00032768224931057434, 'technischen': 0.00020206627137128703, 'verbracht': 0.17926235865960546, 'wirtschaftsraumes': 0.14156986418770295, 'gravierendsten': 0.00556539763354464, 'ru\\\\xdfland': 0.01723578721671475, 'n\\\\xe4mlich': 2.042100146074765e-06, 'besser': 3.869338603508233e-08, 'gro\\\\xdf': 6.818899797702423e-07, 'armee': 1.8672850484177128e-08, 'mitgliedsstaaten': 7.5716764997562245e-06, 'verf\\\\xfcgung': 0.0006546608701682167, 'vielleicht': 7.691558017065464e-11, 'internationalen': 1.0166717905703862e-07, 'pal\\\\xe4stinensischen': 0.000429081308780227, 'nordamerika': 0.00016910681141886407, 'weltweite': 5.062733130797489e-06, '30.': 0.00044601409105432695, 'raum': 0.0004802418847684165, 'verzeichnete': 0.08657609922949983, 'kontinents': 0.00026915728066341423, 'wurden': 2.3525086731606115e-08, 'argumentierten': 0.04798733985730917, 'of': 1.8473856468149426e-06, 'allerdings': 2.5460827946932953e-08, 'alternativvorstellung': 0.06171571021150153, 'wirtschaftsraum': 0.15277846345500115, 'or': 0.04370734596561959, 'weiterbestehen': 0.10944188675423058, 'rolle': 2.5418119213240877e-08, 'jerusalem': 7.931104867611649e-06, 'gruppe': 8.582814076145716e-06, 'area': 0.3339772412136187, 'fast': 1.2042001881511667e-07, 'vorbringen': 0.10715206248571314, 'irlands': 0.0047200752873646805, 'afrika': 2.479493954778853e-10, 'lage': 3.129200035700126e-06, 'zugeschnitten': 0.02333910097157857, 'mazedonien': 0.00204800955774823, 'chilenischen': 0.025935110741906336, 'bescheren': 0.022006837559412087, 'abyei': 0.09707683355773049, 'rahmenbedingungen': 0.11832254922629493, 'gibt': 1.4513560137720004e-08, 'demokratischer': 3.2560571243868327e-06, 'bereiche': 0.003631608839839471, 'gesetzlichen': 0.04077773716321704, 'stabilit\\\\xe4t': 3.4949901947309304e-09, 'st\\\\xe4rkung': 0.0010521162674886576, 'kn\\\\xfcpfte': 0.10944188675423058, '90er': 0.004989706698539122, 'internationale': 1.1440153881156353e-10, 'erde': 0.00042222378132109886, '\\\\u201eout': 0.10944188675423058, 'selbstverst\\\\xe4ndlich': 5.492906111666653e-06, 'harten': 0.0021268076789716294, '150': 3.945793180537351e-07, 'ab': 9.471964997930455e-07, 'verwenden': 0.0007781549816998002, 'zudem': 4.710861785628576e-06, 'interesse': 1.4430253930485474e-07, 'innerhalb': 3.169314229502166e-06, 'fachgebiet': 0.34460932231445207, '\\\\xfcber': 2.5953655714144036e-06, 'vordringlicher': 0.17858823769048437, 'harvard-\\\\xf6konomen': 0.035044730588485666, 'sommer': 1.2346022384244527e-06, 'handel': 3.5812184915371564e-05, 'beide': 4.0602699611558495e-06, 'schlagwort': 4.107284169894235e-05, 'wasserversorgung': 0.0014883319814960607, 'juni': 1.8169671765877722e-08, '\\\\xb7': 0.00014331590668114965, 'prozent': 3.693623376318633e-05, 'spielt': 4.319281475903566e-05, 'radikale': 2.379232201917253e-06, 'geschichte': 2.115057433290662e-09, 'feld': 0.002020806017905085, 'land': 2.3753750673436347e-11, 'euroland': 0.10530582876397525, 'landoberfl\\\\xe4che': 0.13505095329858016, '2000': 1.0319617048710151e-08, 'bewirtschaftete': 0.06059668585656223, '2004': 2.038506132025125e-06})\n",
      "defaultdict(<function <lambda> at 0x000000011E62E9E8>, {'called': 1.1067132475808531e-07, 'sudan\\\\u2019s': 0.08984162806871028, 'bomb': 0.00020242946014071466, 'charles': 2.1047167168423144e-05, 'rest': 4.596994147957301e-05, 'years': 4.099824677847378e-11, 'planning': 0.0014894046155401837, 'baath': 0.1128107862488886, 'go': 3.4307693231649624e-08, 'still': 2.2324796619892242e-10, 'avenue': 0.0020680602739909333, 'move': 1.0128048080392517e-06, 'whose': 1.5582308243107804e-05, 'touches': 0.2283436204640005, 'cut': 1.918906429198757e-06, 'possessions': 0.11081043208946491, 'lille': 0.1110516100901421, 'pinochet\\\\u2019s': 0.03475724616869239, 'surprises': 0.16708174513245944, 'royal': 1.2174348923261337e-06, 'young': 1.105919190429721e-06, 'viewers': 0.00037261747266654317, 'easy': 8.484725978912982e-09, 'prison': 0.0004207411254623929, 'set': 1.4868411815796291e-05, 'save': 6.625422644125089e-07, 'ought': 4.228128053067196e-05, 'happened': 5.329419904210227e-08, 'division': 0.0002852792139571873, 'evening': 0.013715010406802392, 'return': 2.921310166442144e-06, '\\\\u201cwe': 0.002053686452341059, 'artist\\\\u2019s': 0.12977965397940733, 'means': 0.00017824632482238116, 'hamoodi': 0.1128107862488886, 'de': 1.5560842838975482e-08, 'television': 7.402270154819292e-07, 'augusto': 0.0027255799924731246, '120': 8.44393952225165e-05, 'arab-dominated': 0.14496423073444342, 'decide': 6.013869834373482e-06, 'hundred': 0.00013220724565908066, 'world': 7.769610213600174e-11, 'part': 2.1914818480874058e-10, 'depicts': 0.0067834089051261445, 'bank': 6.056889449138126e-07, 'desire': 0.0030179394983902382, 'lincoln': 0.005706929184948931, 'lost': 4.061575684634773e-07, 'covering': 0.009020052844909603, 'kennedy': 3.298562167909657e-05, 'try': 4.961247310761106e-06, 'garage': 0.0018389401141217165, 'journalist': 0.004071752473825172, 'team': 1.7515638128035546e-06, 'gaulle': 1.836279352533826e-06, 'found': 4.330278129703088e-06, 'works': 5.695206511679259e-07, 'sewage': 0.014440374830501448, 'secretary': 1.665160773121357e-05, 'magicians': 0.010229511293939621, 'right': 3.465416483569332e-08, 'people': 2.2199038049092833e-08, 'house': 0.5506332691582514, 'belonged': 0.004251056483110104, 'iran\\\\u2019s': 5.027006623836881e-06, 'back': 5.6064113234886805e-09, 'rivers': 9.656386038264386e-06, 'born': 2.0202620850245655e-07, 'oil': 1.1462086854164291e-08, 'election': 0.0011518489814812396, 'year': 1.243518489057068e-06, 'proper': 0.00010968718233337982, 'home': 0.01784696498700327, 'happen': 8.38199823495151e-09, 'even': 4.2826741461040866e-09, 'afghanistan': 0.0007136806070210318, 'scale': 0.0004927988526319387, 'recently': 9.18591080242695e-07, 'charges': 0.0015677747446553641, 'sudan': 3.8889736197898e-05, 'away': 1.0954630946365845e-06, None: 2.2781098916881503e-08, 'enough': 8.844602715142482e-09, 'new': 1e-12, 'dominate': 0.000734976259165425, 'independence': 4.740653910356926e-07, 'solitaire': 0.1110516100901421, 'nicolas': 0.0011524987763647102, 'president\\\\u2019s': 0.00786123481513746, 'christian': 1.1954149135062106e-06, 'power': 1.0759779869909227e-11, 'men': 6.072247461997386e-06, 'lapping': 0.014440374830501448, 'supposedly': 0.00037837970841786287, 'french': 9.776837807976196e-08, 'water': 4.7892661642816346e-08, 'control': 0.00010797623109808348, 'leadership': 1.9296824055421232e-07, 'members': 7.484962946094799e-08, 'others': 9.761703199410142e-08, 'buy': 2.839434141874276e-06, 'debt': 3.340276832445831e-06, '\\\\u2013': 1e-12, 'undermine': 4.679417351305554e-05, 'displaced': 0.00149454621023222, 'jos\\\\xe9': 0.0001268088129214462, 'accident': 0.0007886790522238365, 'many': 1e-12, 'could': 1.673618622198693e-10, '30': 2.3379289173549024e-08, 'days': 1.9400481127787773e-07, 'leaving': 0.007903845909194579, 'stand': 3.069870671986975e-06, 'faces': 0.04449043131670024, 'argue': 6.337734821738561e-06, 'jersey': 3.619541828195893e-05, 'otherwise': 0.00046967631763847735, 'sentenced': 0.008934792824551945, 'live': 3.314902062769078e-07, 'opposition': 9.536200721887515e-07, 'family': 0.0003741090040052149, 'suddenly': 7.204249938130261e-05, 'betting': 0.0004964916879625531, 'kilometers': 1.78755993875605e-07, 'anticipated': 0.05081221039773125, 'one': 5.556505081901038e-07, 'smaller': 1.608488820672412e-05, 'man\\\\u201d': 0.04471630540534673, 'electronic': 0.00045256284575588784, 'crisis': 1.4411008075393e-08, 'squads': 0.11398955323690671, '1986': 4.8999318632262556e-06, 'carrasco': 0.11398955323690671, 'give': 7.505905787784338e-07, 'two': 2.036087913263384e-09, 'governance': 0.0010720035185585633, 'next': 9.632293023133507e-06, 'anyone': 2.6844108310772744e-05, 'attack': 2.3553629409808253e-07, 'vehicle': 0.00044682740801407365, 'white': 0.01492408066445226, 'john': 8.85229652734338e-08, 'tell': 8.458933644210305e-06, 'store': 0.016828364675152346, 'spacious': 0.01988742128606022, 'expiring': 0.1110516100901421, 'hanover': 0.03884545020758645, '``': 2.2993029044449565e-11, 'educated': 0.00034428930691911925, 'door': 2.7928383559859927e-05, 'fire': 0.005904013890303938, 'thousand': 0.002883718112693602, 'treasury': 2.8938544856638773e-05, 'sister': 0.09909931721056318, 'darfur': 0.00010770497207241861, 'somehow': 0.00046685650555856493, 'fund': 0.0004744150568564286, 'kids': 3.700444377408172e-05, 'chile': 1.0572478720072425e-05, 'foundations': 0.00014455578304283927, 'trying': 0.00028681155863292425, 'institution': 9.103787557868324e-06, 'charity': 0.040294549657824696, 'begins': 0.10583810312722869, \"''\": 1.4255544621273758e-11, 'none': 1.8004468333065905e-06, 'october': 9.618889246314367e-08, 'bid': 0.0973904443311654, 'bach\\\\u2019s': 0.02239987760600222, '16': 4.140724953267276e-06, 'whether': 1.2815371019185826e-06, 'laid': 0.0021989019929477822, 'us': 1e-12, 'nations': 6.894903128141209e-05, 'advisors': 0.0006355313465336583, '\\\\u201ctexas': 0.04471630540534673, 'colombey-les-deux-\\\\xe9glises': 0.1110516100901421, 'homes': 0.048026547240609334, 'ruling': 0.00018388521448115273, 'caveat': 0.010229511293939621, '1962': 4.2174251976347136e-05, 'example': 3.374341769558175e-10, 'friis': 0.02239987760600222, 'heart': 3.591995102693643e-07, 'palace': 0.0004736654274916853, 'played': 1.2235624968644073e-07, 'general': 2.890123279008806e-07, 'respected': 0.005152425522136034, 'raw': 0.0024310826105238843, 'good': 2.1961190140721422e-07, 'need': 1.3162638113732454e-08, 'hit': 3.5203282910746616e-05, 'electric-powered': 0.02239987760600222, 'conserve': 0.004009168530866836, 'f.': 0.0001741843386287511, 'affairs': 0.05170285297479515, 'genuine': 0.00036367283770520934, 'democracy': 1.3077090633243675e-10, 'provide': 6.768328384398967e-07, 'allow': 2.12772236533196e-05, 'divided': 0.00036841735533428105, 'defendants\\\\u2019': 0.1415217139079909, '40': 6.14101652246182e-08, 'fortunate': 0.0013601719753726321, 'globalization': 0.0002064706528847248, 'parts': 1.7710611626251726e-08, 'film': 3.324674771271191e-05, 'doors': 0.003343640687939748, 'party': 5.48969712686413e-09, 'won\\\\u2019t': 1.3946413088766987e-06, 'santiago': 0.000611221724031686, 'putin': 1.1369045335110492e-06, 'blocks': 0.0010988814609961524, 'styling': 0.03884545020758645, 'closest': 0.001396843184117409, 're-election': 0.007193554452947113, 'stakes': 0.0002070766724375608, 'files': 0.06326820245121108, 'tapia': 0.11398955323690671, 'running': 1.204956869002173e-06, 'said': 0.0005423575070623078, 'president': 6.146439513649312e-08, 'sarkozy\\\\u2019s': 0.004166613779083141, 'died': 6.916930398786777e-07, 'pull': 0.059061579732695865, 'ago': 5.0329490735657383e-08, 'death': 2.09287182981878e-07, 'longer': 3.542016038717828e-05, 'pennsylvania': 0.0008306309735705345, 'maybe': 6.703589113365048e-05, 'later': 0.0002109239233412757, 'remain': 9.226042996418593e-07, 'together': 4.743882064079076e-07, 'face': 1.3306923089473111e-06, 'beds': 0.0015622613048899142, 'without': 9.750115497029709e-09, 'south': 2.2456209008646632e-07, 'incapable': 0.00012313385608495015, 'time': 5.774809180792244e-11, 'organization': 1.573361361886203e-05, 'abraham': 0.009053130845131931, 'dragged': 0.003001887652940408})\n"
     ]
    }
   ],
   "source": [
    "print tm_en_de['area']\n",
    "print tm_de_en['haus']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "e0c227e0dc07246a4c21590b3a351c7b89c1cc4d195baca9fae0a971"
   },
   "source": [
    "### Decoding \n",
    "\n",
    "Finally you'll need to solve for the best translation for a given string. For this you should do simple word-for-word translation where each word is translated independently. Use the alignments learned above (or the translation parameters) to translate each word of the queries into English. Compare taking the maximum probability of translation into English $p(e|f)$, with the noisy-channel probability, $p(f|e)p(e)$, which considers the reverse translation and a unigram language model probability. (You'll get a chance later to do something more ambitious, using the trigram model.)\n",
    "\n",
    "Use your algorithms to translate the first 100 queries into English, and save them to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "signature": "059a2c0ace59320fd4409ea351eb6e0d8fd7bcc33cbd3fb7f5b6b8ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram Perplexity with Good Turing Smoothing: \n",
      "6932.66847706\n"
     ]
    }
   ],
   "source": [
    "from decimal import *\n",
    "\n",
    "def get_log_uni_prob_gt(token, unigram_counts, value_counts, token_count, k):\n",
    "    c = unigram_counts.get(token, 0)\n",
    "    nc = value_counts.get(c, 1)\n",
    "    nc1 = value_counts.get(c+1, 1)\n",
    "    if c != 0:\n",
    "        c_star = c\n",
    "    else:\n",
    "        c_star = (c+1)*nc1/nc\n",
    "    p = Decimal(c_star)/Decimal(token_count)\n",
    "    if k == 1:\n",
    "        return float(c_star)/float(token_count)\n",
    "    else:\n",
    "        return p.ln()\n",
    "\n",
    "def get_sent_uni_log_prob_gt(sentence, unigram_counts, value_counts, token_count):\n",
    "    sentence = convert_sentence_test(sentence, unigram_counts)\n",
    "    return sum([get_log_uni_prob_gt(sentence[i], unigram_counts, value_counts, token_count, 5) for i in range(0,len(sentence))])\n",
    "\n",
    "def calculate_uni_perplexity(sentences,unigram_counts,token_count, smoothing_function):\n",
    "    value_counts = collections.Counter(unigram_counts.values())\n",
    "    total_log_prob = 0\n",
    "    test_token_count = 0\n",
    "    for sentence in sentences:\n",
    "        test_token_count += len(sentence)+1 # have to consider the end token\n",
    "        total_log_prob += smoothing_function(sentence,unigram_counts, value_counts, token_count)\n",
    "    return math.exp(-total_log_prob/test_token_count)\n",
    "\n",
    "print \"Unigram Perplexity with Good Turing Smoothing: \"\n",
    "print calculate_uni_perplexity(test_set,docs_unigrams1,docs_token_count1, get_sent_uni_log_prob_gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "def get_de_query_list(doc_path):\n",
    "    querys = []\n",
    "    query_id_list = []\n",
    "    with open(doc_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            doc_id, text = line.split('\\t')\n",
    "            querys.append(preprocess_de(text))\n",
    "            query_id_list.append(doc_id)\n",
    "    return query_id_list, querys\n",
    "\n",
    "de_query_id, de_query_list = get_de_query_list(\"./dev.queries\")\n",
    "\n",
    "def decode(query, translation_model, unigram_counts, bigram_counts, trigram_counts, token_count, n):\n",
    "    value_counts = collections.Counter(unigram_counts.values())\n",
    "    final_result = []\n",
    "    for de_token in query:\n",
    "        string = \"\"\n",
    "        prob = -999999\n",
    "     \n",
    "        for token, value in translation_model[de_token].iteritems():\n",
    "            if((token != 'None') and token not in english_stop_words):\n",
    "                uni_log_prob = get_log_uni_prob_gt(token, unigram_counts, value_counts, token_count, 1)\n",
    "                prob_tmp = value * uni_log_prob\n",
    "                if prob_tmp > prob:\n",
    "                    prob = prob_tmp\n",
    "                    string = token\n",
    "        if string != \"\" and string != '``':\n",
    "            final_result.append(string)\n",
    "    return final_result\n",
    "\n",
    "querys = de_query_list[0:100]\n",
    "outcomes = []\n",
    "for query in querys:\n",
    "    outcomes.append(decode(query, tm_de_en, docs_unigrams1, docs_bigrams, docs_trigrams, docs_token_count1, 1))\n",
    "\n",
    "import pickle\n",
    "def load_object(file_path):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        object_instance = pickle.load(f)\n",
    "    return object_instance\n",
    "\n",
    "def saving_object(file_path, object_instance):\n",
    "    with open(file_path, 'wb') as f:\n",
    "        pickle.dump(object_instance, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "print len(outcomes)\n",
    "saving_object('./translated_100.en', outcomes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "1a18c3f5dacbbcc943d1f06ddac3c36f085c9e9beffb399c9565bbf5"
   },
   "source": [
    "Now you will need to write some text about the procedure above, and how well it worked. You should answer:\n",
    "  - how good are the translations? How do your methods fare at translating the content words versus functions words? You can use Google translate to provide an alternative (pretty good) translation for comparison purposes. \n",
    "  - what are good settings for various modelling parameters such as language model discounting, translation model smoothing, any decoding parameters, and how do these affect the outputs?\n",
    "  - compare the language model perplexity for the unigram and trigram models over your decoder outputs, how does the difference in perplexity between the two models compare to your test above on the monolingual data? \n",
    "  - how do the learned alignments differ between the two translation directions, and does their combination appear oto improve the predictions? (You don't need a formal evaluation here, as we have no *gold standard* alignments.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "signature": "023387c5d98e0b9a5d2b17458adbe52f1a67c1204c9393b4ebd1440e"
   },
   "source": [
    "For single query, the translator works well. It sometime can return a meaningful outcome. For example, searching for \"verliert\", it can return \"loses\", and \"verbindungen\" for \"ties\", although, tie is a uncommon translation of \"verbindungen\". For quite much test words or artifatically selected words, the translation model can give a pretty good translation, only if the word is a content word. Because of the stop word removal machanism, it cannot return translation for any functions words I tested.\n",
    "\n",
    "For the trigram language model with interpolation smoothing method, I implemented above. There are 2 lambdas. For the selection of lambda, I experiment for a range of lambda for bigram and trigram. I found that when the lambdas are both 0.25, the perplexity is the least. For the unigram, the good turing smoothing does not have any parameter that can impact the final outcome.The perplexity is extremely high, which is aproximately 6932. It is much higher than trigram's.\n",
    "\n",
    "The learned alignments performed pretty well. By observing the outcomes, their combination appears to improve the predictions. For the simple implementation, I only use a beginner alignments machanism."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "d4e6497008c47c701f6ace19160ee57f7477ce70fd9b28ebeca539d1"
   },
   "source": [
    "## Putting the CLIR system together (10% of assignment mark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "signature": "f2f0d98a97c28176358535df76bd9655dd367ad62cfaf1b54292ed74"
   },
   "source": [
    "Now you should couple the translation and IR components from above. Take your translated queries and use these with the IR system to find the most relevant documents. You should then evaluate these predictions against the supplied relevance judgements, using the *mean average precision* (MAP) metric. As part of this, you will want to consider tuning the settings of the IR system for best performance, and comment on how successful your approach was and evaluate the IR performance is affected by the parameter settings and modelling decisions you have made.\n",
    "\n",
    "Note that if you were stuck on the translation steps above, or your translations are otherwise unusable, you can implement this step using google translate outputs for the 100 queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_query_result_dict(doc_path):\n",
    "    query_result_dict = {}\n",
    "    with open(doc_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            query_id, pad, result_id, score = line.split('\\t')\n",
    "            query_result_dict[query_id] = query_result_dict.get(query_id, []) + [result_id]\n",
    "    return query_result_dict\n",
    "\n",
    "query_result = get_query_result_dict('./dev.qrel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "signature": "217b9149695dd0d5a18be4ea3ce8264785637a99c8b3538b333ccb50"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5 0.25\n",
      "0.000666090392155\n",
      "0.5 0.5\n",
      "0.000665436797383\n",
      "0.5 0.75\n",
      "0.000668371425135\n",
      "0.5 1.0\n",
      "0.00066619835086\n",
      "0.75 0.25\n",
      "0.00068795266759\n",
      "0.75 0.5\n",
      "0.000682164438851\n",
      "0.75 0.75\n",
      "0.000683454563952\n",
      "0.75 1.0\n",
      "0.000678638292046\n",
      "1.25 0.25\n",
      "0.000754133422388\n",
      "1.25 0.5\n",
      "0.000750186411187\n",
      "1.25 0.75\n",
      "0.000711724690504\n",
      "1.25 1.0\n",
      "0.000727840535918\n",
      "1.5 0.25\n",
      "0.000748861923105\n",
      "1.5 0.5\n",
      "0.000756516966666\n",
      "1.5 0.75\n",
      "0.000726111721072\n",
      "1.5 1.0\n",
      "0.000696164878114\n",
      "2.0 0.25\n",
      "0.000752011766752\n",
      "2.0 0.5\n",
      "0.000753459551299\n",
      "2.0 0.75\n",
      "0.000729490982035\n",
      "2.0 1.0\n",
      "0.000694216382186\n"
     ]
    }
   ],
   "source": [
    "tranlated_querys = load_object('./translated_100.en')\n",
    "test_de_query_id = de_query_id[0: 100]\n",
    "\n",
    "def get_query_precision(query_result_id, clir_results_index):\n",
    "    query_result_id_set = set(query_result_id)\n",
    "    tp = 0\n",
    "    for index in clir_results_index:\n",
    "        if posting_list_generator.doc_id_list[index] in query_result_id_set:\n",
    "            tp += 1\n",
    "    return float(tp)/len(query_result_id)\n",
    "\n",
    "def get_sent_map(query_id, query_result, clir_results):\n",
    "    sum_precision = 0.0\n",
    "    for i in range(len(query_id)):\n",
    "        precision = get_query_precision(query_result[query_id[i]], clir_results[query_id[i]])\n",
    "        sum_precision += precision\n",
    "    return sum_precision/len(de_query_id)\n",
    "\n",
    "def display_tuning(kk1, bb):\n",
    "    bm25model = BM25(posting_list_generator, k1 = kk1, b=bb)\n",
    "    clir_results = {}\n",
    "    for i in range(len(tranlated_querys)):\n",
    "        relevant_doc_id = query_result[de_query_id[i]]\n",
    "        num_results = len(relevant_doc_id)\n",
    "        result_dict = bm25model.get_sent_score(tranlated_querys[i])\n",
    "        sorted_result = sorted(result_dict.items(), key=operator.itemgetter(1), reverse = True)\n",
    "        selected_clir_results = dict(sorted_result[0:num_results])\n",
    "        clir_results[test_de_query_id[i]] = selected_clir_results.keys()\n",
    "        \n",
    "    print get_sent_map(test_de_query_id, query_result, clir_results)\n",
    "\n",
    "for k1 in [0.5, 0.75, 1.25, 1.5, 2.0]:\n",
    "    for b in [0.25, 0.5,0.75, 1.0]:\n",
    "        print k1, b\n",
    "        display_tuning(k1, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "saving_object('./translated_100.clir', clir_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "48cadf73210cba1965c1947cb3fc41ee1114f5ac66d2bbc24d90136a"
   },
   "source": [
    "The precision is the number of true positive divided by the number of true positive and false positive. However, the threshold or cretiria of relevance(how large the bm25 score should be means relevance)is unclear. So in the final test, I choose the number of CLIR result with most bm25 score same as the number of results read from dev.qrel. It will affact the final outcome. I assumed that the impact of this will be tiny. And the tuning of the IR module shows that when k1 is 1.5 and b is 0.5, the system gets the best mean average precision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "6e7202f920fcf0c884903cca397e8f53505a87211d1469cbeda397dd"
   },
   "source": [
    "Note that in a complete CLIR you would translate the retreived documents back into German. We won't bother about that for this assignment, especially as I doubt many of you can understand German."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "cd1b5c7fad5a61a75ff7eb8782c0ea9b423f67487b1dae0961a35252"
   },
   "source": [
    "# Part 2: Extension and discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "1e9fc7d99a2189f5a92d950a1571a568609ac0ad2e97ae8f8104f9fa"
   },
   "source": [
    "## Extension (25% of assignment mark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "30363bea9f406c5bdfd612214be2081ee74d24895a0a20c67f7cf21a"
   },
   "source": [
    "You now have a working end-to-end CLIR system. The next step is to revisit some of the steps above to see if you can improve the performance of the system. You are invited to be creative with your ideas about how to do this. Note that many great ideas might not produce improvements, or end up being very difficult to implement. This will not be assessed solely on the grounds of CLIR accuracy; we will be looking primarily for interesting and creative ideas.\n",
    "\n",
    "Here are some ideas on what you might do. Warning: some are bigger than others, aim to spend 5-6 hours in total on this.\n",
    "\n",
    "#### Structured decoding\n",
    "\n",
    "Decode using a higher order language model, possibly with word reordering. Using your trigram language model and a word-based or phrase-based translation model, search for the best scoring translation sentence. You may want to use the tools in *nltk.translate*, including the phrase extraction and *StackDecoder*, especially if you wish to support word reordering. The decoding algorithm is much simpler (and tractable) without reordering, so you might want to implement this yourself. You will want to work with log-probabilities to avoid numerical underflow. Note  that the *StackDecoder* is not highly stable code, so you will need to understand its code in detail if you chose to use it. You will need to supply translation log probabilities and language model log probabilities as input to the decoder and consider the context of the previous two words in estimating the probability of each word. \n",
    "\n",
    "#### Probabilisitc querying\n",
    "\n",
    "Consider queries including weighted terms based on multiple translation outputs, such as the word translation probabilities or using *k-best* translations from a decoder (you will need to extract not just the best translation from the decode, but the runners up, which can be done from the stack contents after beam search.)  You should consider how best to define TF and DF for query words based on their ambiguous translations. You may find inspiration in the bibliography of [Douglas Oard](https://terpconnect.umd.edu/~oard/research.html#mlia) such as [this paper](https://terpconnect.umd.edu/~oard/pdf/coling12ture.pdf).\n",
    "\n",
    "#### Word alignment\n",
    "\n",
    "Implement an extended word alignment model, such as the *hmm* model (see [Vogel's paper](http://www.aclweb.org/anthology/C96-2141)) or a variant of *ibm2*. The variant of *ibm2* could include a better formulation for the distortion term, $p(i|j,I,J)$, based around rewarding values of i and j that are at similar relative positions to encourage alignments near the diagonal. This contrasts with *ibm2* which just learns a massive table of counts for all combinations of i,j,I and J. This strictly needs normalisation, as its not a probability, but for simplicity you can ignore this aspect here. This idea is inspired by [fast-align](https://github.com/clab/fast_align) which has a similar, but slightly more complex, formulation.\n",
    "\n",
    "You might also want to experiment with [fast-align](https://github.com/clab/fast_align) (an extremely fast and accurate variant of *ibm2*) or [GIZA++](http://www.statmt.org/moses/giza/GIZA++.html) (an implementation of *ibm1* - *ibm5* and the *hmm*), both widely used and highly optimised alignment tools. Neither of these tools are in python, and may be a little involved to install and compile.\n",
    "\n",
    "#### Decoding\n",
    "\n",
    "Experiment with a state-of-the-art translation tool like [Moses](http://www.statmt.org/moses/).  Can you get better CLIR performance using this, trained on the full parallel dataset? You might want to consider translating the document collection into German, and then doing the IR entirely in German. You might consider building a python interface to moses, building on the existing very basic funcationality. **Warning:** Moses is a complex suite of software, and can be a little complex to install and use. However there are good installation and usage guides on the moses webpage.\n",
    "\n",
    "#### IR engine\n",
    "\n",
    "There are several great open source IR engines, including [Lucene](https://lucene.apache.org/core/), [Terrier](http://terrier.org/), [Zettair](http://www.seg.rmit.edu.au/zettair/) and [Lemur](http://www.lemurproject.org/). You could experiment with these, and use this to compare the accuracy of different types of IR models on the CLIR data. Alternatively, you could implement a faster or more compact retreival system in python, e.g., using compressed vbyte encodings, skip lists or similar.\n",
    "\n",
    "Note that the development of some of the extensions maye require working in the command shell and in other languages than python. If this is the case, you should attach your code in separate files as part of your submission, but include the analysis text here. *Note that excellent submissions implementing interfaces or models not currently in NLTK will be considered for inclusion into the toolkit.* You should include both the code, and text explaining your work and findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "signature": "13fb1e605b994adccc0faa8943a3f3fca7779e22babb6fd3e32a7993"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "4362c728f9478b4c8b891dd0a7a588402f9ae4e351fadcb6fd45b6df"
   },
   "source": [
    "```your supporting description analysis goes here```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "b31b7fabf9b858db14b2d96816a45a66cac96474bc26dc5783867685"
   },
   "source": [
    "## Discussion (5% of marks)\n",
    "\n",
    "What conclusions can you make about CLIR, or more generally translation and IR? Overall what approaches worked and what ones didn't? What insights do you have from doing this, put another way, if you were to start again, what approach would you take and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "signature": "59a9457760698a5efdc199a54dfda0242b766880dec9d9b71ef774f3"
   },
   "source": [
    "Until here, the CLIR is finally completed and with a reasonable result set, although it is a beginner translation system. Many results cannot be a good translation even comparing to it given by Google Translation. Machine learning is based on CLIR, which takes an source query first, and then uses a range of language models, translation models and decoding methods to get a result in target language. This result is not a final work. It should still be retrieved in a huge size of corpus data. Due to the limitation of time, this CLIR system can not be well considered and constructed. For example, there are more effective decoding method, and also a range of methods of selecting threshold of relevance. If I start this again, I will use translation model with word order instead of ibm1, which has no word order. And trigram decoding method. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
